{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c434a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "path_to_data = \"/home/amnnrz/GoogleDrive - msaminnorouzi/PhD/Projects/DSFAS/Data/\"\n",
    "df = pd.read_csv(path_to_data + \"Carbon&satellite_data_joined_v1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepthSampl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df\n",
    "depth_0_12_idx = dff.DepthSampl == \"0_12\"\n",
    "print(depth_0_12_idx.value_counts())\n",
    "dff.loc[depth_0_12_idx, \"rrrr\"] = dff.loc[depth_0_12_idx,\n",
    "                                          \"TotalC\"]/100 * 12 * 1 * 2.54 * df.loc[depth_0_12_idx, \"BD_g_cm3\"]\n",
    "dff.rrrr.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_first = pd.read_csv(\n",
    "#     \"/Users/aminnorouzi/Library/CloudStorage/GoogleDrive-msaminnorouzi@gmail.com/My Drive/PhD/Projects/DSFAS/df_first.csv\", index_col = 0)\n",
    "# df_second = pd.read_csv(\n",
    "#     \"/Users/aminnorouzi/Library/CloudStorage/GoogleDrive-msaminnorouzi@gmail.com/My Drive/PhD/Projects/DSFAS/df_second.csv\", index_col= 0)\n",
    "# df_first = df_first.loc[df_first[\"DepthSampl\"] == \"0_12\"].copy()\n",
    "# df_second = df_second.loc[df_second[\"DepthSampl\"] == \"0_12\"].copy()\n",
    "\n",
    "# \"total_c_%\" /100 * height * A * 2.54 (inch to cm) * BD\n",
    "df[\"Total_C_g/cm2\"] = df[\"TotalC\"]/100 * 12 * 1 * 2.54 * df[\"BD_g_cm3\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f176b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepthSampl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ccffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_df = df.loc[df.SampleID.duplicated(keep=False)]\n",
    "dup_df\n",
    "\n",
    "averaged_C = pd.DataFrame([])\n",
    "averaged_C['SampleID'] = dup_df.SampleID.unique()\n",
    "for id in dup_df.SampleID.unique():\n",
    "    averaged_C.loc[averaged_C[\"SampleID\"] == id, \"Total_C_g/cm2\"] = np.mean(dup_df.loc[dup_df[\"SampleID\"] == id, \"Total_C_g/cm2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db846e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_C['Total_C_g/cm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d372404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[~df.SampleID.duplicated()]\n",
    "df1.loc[df1.SampleID.isin(averaged_C.SampleID), 'Total_C_g/cm2'] = averaged_C['Total_C_g/cm2'].values\n",
    "df1.loc[df1.SampleID.isin(averaged_C.SampleID), 'Total_C_g/cm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032349de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize band values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# assuming df is your pandas dataframe\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select the columns you want to normalize\n",
    "cols_to_normalize = ['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second']\n",
    "\n",
    "# fit the scaler on the selected columns\n",
    "scaler.fit(df[cols_to_normalize])\n",
    "\n",
    "# transform the selected columns to have zero mean and unit variance\n",
    "df[cols_to_normalize] = scaler.transform(df[cols_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea45169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[:, 8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase the font size of the labels\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Increase the resolution of the plot\n",
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "# Plot the density distribution of column 'Total_C_g/cm2'\n",
    "df1['Total_C_g/cm2'].plot(kind='density')\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel('Total C (g/cm$^2$)', fontsize=14)\n",
    "\n",
    "# Mark actual values on the curve\n",
    "min_value = df1['Total_C_g/cm2'].min()\n",
    "max_value = df1['Total_C_g/cm2'].max()\n",
    "\n",
    "# Plotting the actual values on the curve\n",
    "plt.axvline(x=min_value, color='red', linestyle='--', label='Min')\n",
    "plt.axvline(x=max_value, color='blue', linestyle='--', label='Max')\n",
    "\n",
    "# Display legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first['Total_C_g/cm2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second', 'Total_C_g/cm2']\n",
    "\n",
    "df = df1[selected_cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4265de8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df.nunique()[df.nunique() == 1].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767db65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your data into a Pandas DataFrame\n",
    "df = df1[selected_cols].copy()\n",
    "\n",
    "# Drop columns with just one value\n",
    "# df.drop(columns= df.nunique()[df.nunique() == 1].index[0], inplace=True )\n",
    "\n",
    "# Set the name of your y-variable\n",
    "y_var = 'Total_C_g/cm2'\n",
    "\n",
    "# Set the terciles to use for separating the data\n",
    "bottom_tercile = np.percentile(df[y_var], 33.33)\n",
    "top_tercile = np.percentile(df[y_var], 66.66)\n",
    "\n",
    "# Create a new column in the DataFrame to indicate whether each row is in the top, middle, or bottom tercile\n",
    "df['tercile'] = pd.cut(df[y_var], bins=[df[y_var].min(\n",
    "), bottom_tercile, top_tercile, df[y_var].max()], labels=['bottom', 'middle', 'top'])\n",
    "\n",
    "# Loop through each x-variable and create a density distribution plot for the top, middle, and bottom terciles\n",
    "for x_var in df.columns.drop([y_var, 'tercile']):\n",
    "    g = sns.FacetGrid(df[df['tercile'] != 'middle'], hue='tercile', height=4, aspect=1.2)\n",
    "    g.map(sns.kdeplot, x_var, shade=True)\n",
    "    g.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df['tercile'] == 'top']['Total_C_g/cm2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df1['Total_C_g/cm2']\n",
    "X = df1[['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second']]\n",
    "# X = df_second[['B12_second']]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d42e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr_matrix = df[['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second']].corr()\n",
    "\n",
    "\n",
    "# plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second', 'Total_C_g/cm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1[selected_cols]\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your data into a Pandas DataFrame\n",
    "df = df1[selected_cols]\n",
    "\n",
    "# # Drop columns with just one value\n",
    "# df.drop(columns= df.nunique()[df.nunique() == 1].index[0], inplace=True )\n",
    "\n",
    "# Set the name of your y-variable\n",
    "y_var = 'Total_C_g/cm2'\n",
    "\n",
    "# Set the terciles to use for separating the data\n",
    "bottom_tercile = np.percentile(df[y_var], 33.33)\n",
    "top_tercile = np.percentile(df[y_var], 66.66)\n",
    "\n",
    "# Create a new column in the DataFrame to indicate whether each row is in the top, middle, or bottom tercile\n",
    "df['tercile'] = pd.cut(df[y_var], bins=[df[y_var].min(\n",
    "), bottom_tercile, top_tercile, df[y_var].max()], labels=['bottom', 'middle', 'top'])\n",
    "\n",
    "# Loop through each x-variable and create a density distribution plot for the top, middle, and bottom terciles\n",
    "for x_var in df.columns.drop([y_var, 'tercile']):\n",
    "    g = sns.FacetGrid(df[df['tercile'] != 'middle'], hue='tercile', height=4, aspect=1.2)\n",
    "    g.map(sns.kdeplot, x_var, shade=True)\n",
    "    g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12759f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_second['Total_C_g/cm2']\n",
    "X = df_second[['NDVI_second', 'tvi_second',\n",
    "               'savi_second', 'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "               'MSAVI2_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "               'CI_second',\n",
    "               'B2_second', 'B3_second', 'B4_second', 'B8_second', 'B11_second',\n",
    "               'B12_second']]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load your data into a Pandas DataFrame\n",
    "df = df1[selected_cols]\n",
    "\n",
    "# Set the name of your y-variable\n",
    "y_var = 'Total_C_g/cm2'\n",
    "\n",
    "# Set the terciles to use for separating the data\n",
    "bottom_tercile = np.percentile(df[y_var], 33.33)\n",
    "top_tercile = np.percentile(df[y_var], 66.66)\n",
    "\n",
    "# Subset the DataFrame to include only top and bottom tercile rows\n",
    "df_terciles = df[(df[y_var] <= bottom_tercile) |\n",
    "                 (df[y_var] >= top_tercile)].copy()\n",
    "\n",
    "# Create a new column for the target variable ('high' or 'low') based on tercile membership\n",
    "df_terciles['target'] = np.where(\n",
    "    df_terciles[y_var] >= top_tercile, 'high', 'low')\n",
    "\n",
    "# Select only the X variables of interest\n",
    "# Replace with the actual X variable names\n",
    "X_terciles = df_terciles[['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second']]\n",
    "y_terciles = df_terciles['target']\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_terciles, y_terciles, test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(classifier, X_terciles, y_terciles, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the classifier on the entire data\n",
    "classifier.fit(X_terciles, y_terciles)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_terciles)\n",
    "\n",
    "# Generate a contingency table\n",
    "contingency_table = pd.crosstab(y_terciles, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb46eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load your data into a Pandas DataFrame\n",
    "df = df1[selected_cols]\n",
    "\n",
    "# Set the name of your y-variable\n",
    "y_var = 'Total_C_g/cm2'\n",
    "\n",
    "# Set the terciles to use for separating the data\n",
    "bottom_tercile = np.percentile(df[y_var], 33.33)\n",
    "top_tercile = np.percentile(df[y_var], 66.66)\n",
    "\n",
    "# Subset the DataFrame to include only top and bottom tercile rows\n",
    "df_terciles = df[(df[y_var] <= bottom_tercile) |\n",
    "                 (df[y_var] >= top_tercile)].copy()\n",
    "\n",
    "# Create a new column for the target variable ('high' or 'low') based on tercile membership\n",
    "df_terciles['target'] = np.where(\n",
    "    df_terciles[y_var] >= top_tercile, 'high', 'low')\n",
    "\n",
    "# Select only the X variables of interest\n",
    "# Replace with the actual X variable names\n",
    "X_terciles = df_terciles[['NDVI_first', 'tvi_first',\n",
    "       'savi_first', 'MSI_first', 'GNDVI_first', 'GRVI_first', 'LSWI_first',\n",
    "       'MSAVI2_first', 'WDVI_first', 'BI_first', 'BI2_first', 'RI_first',\n",
    "       'CI_first', 'B1_first', 'B2_first', 'B3_first', 'B4_first', 'B8_first',\n",
    "       'B11_first', 'B12_first', 'NDVI_second', 'tvi_second', 'savi_second',\n",
    "       'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "       'MSAVI2_second', 'WDVI_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "       'CI_second', 'B1_second', 'B2_second', 'B3_second', 'B4_second',\n",
    "       'B8_second', 'B11_second', 'B12_second']]\n",
    "y_terciles = df_terciles['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_terciles, y_terciles, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(classifier, X_train, y_train, cv=3)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "test_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=['Actual low', 'Actual high'], columns=['Predicted low', 'Predicted high'])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Print the test score\n",
    "print(\"Test Score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load your data into a Pandas DataFrame\n",
    "df = df_second[selected_cols]\n",
    "\n",
    "# Set the name of your y-variable\n",
    "y_var = 'Total_C_g/cm2'\n",
    "\n",
    "# Set the terciles to use for separating the data\n",
    "bottom_tercile = np.percentile(df[y_var], 33.33)\n",
    "top_tercile = np.percentile(df[y_var], 66.66)\n",
    "\n",
    "# Subset the DataFrame to include only top and bottom tercile rows\n",
    "df_terciles = df[(df[y_var] <= bottom_tercile) |\n",
    "                 (df[y_var] >= top_tercile)].copy()\n",
    "\n",
    "# Create a new column for the target variable ('high' or 'low') based on tercile membership\n",
    "df_terciles['target'] = np.where(\n",
    "    df_terciles[y_var] >= top_tercile, 'high', 'low')\n",
    "\n",
    "# Select only the X variables of interest\n",
    "# Replace with the actual X variable names\n",
    "X_terciles = df_terciles[['NDVI_second', 'tvi_second',\n",
    "                          'savi_second', 'MSI_second', 'GNDVI_second', 'GRVI_second', 'LSWI_second',\n",
    "                          'MSAVI2_second', 'BI_second', 'BI2_second', 'RI_second',\n",
    "                          'CI_second', 'B1_second', 'B2_second', 'B3_second',\n",
    "                          'B4_second', 'B8_second', 'B11_second', 'B12_second']]\n",
    "y_terciles = df_terciles['target']\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_terciles, y_terciles, test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(classifier, X_terciles, y_terciles, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the classifier on the entire data\n",
    "classifier.fit(X_terciles, y_terciles)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_terciles)\n",
    "\n",
    "# Generate a contingency table\n",
    "contingency_table = pd.crosstab(y_terciles, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "print(contingency_table)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "gis_env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
